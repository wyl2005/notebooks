{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/mnist/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0207 09:07:39.058389 139679559546688 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting ./data/mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting ./data/mnist/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets(\"./data/mnist/\",one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float32 uint8\n"
     ]
    }
   ],
   "source": [
    "a, b =mnist.train.next_batch(11)\n",
    "print(a.dtype, b.dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 自定义的卷积模型\n",
    "# 1. 准备数据的占位符 x [None, 784], y_true [None, 10]\n",
    "with tf.variable_scope(\"data\"):\n",
    "    x = tf.placeholder(tf.float32, [None, 784])\n",
    "    y_true = tf.placeholder(tf.int8, [None, 10])\n",
    "    \n",
    "# 2. 一卷积层 卷积5*5*1, 32个， strides=1, 激活tf.nn.relu 池化\n",
    "with tf.variable_scope(\"conv1\"):\n",
    "    # 随机初始化卷积Filter权重 [5, 5, 1, 32]，偏置 [32]\n",
    "    w_conv1 = tf.Variable(tf.random_normal([5,5,1,32],mean=0.0,stddev=1.0),\n",
    "                         name=\"w_conv1\")\n",
    "    b_conv1 = tf.Variable(tf.constant(0.0, shape=[32]),name=\"conv1_b\")\n",
    "\n",
    "    # 对x进行形状改变[None 784] [None, 28, 28, 1]\n",
    "    x_reshape = tf.reshape(x, [-1, 28, 28, 1])\n",
    "    \n",
    "    # [None,28,28,1] --> [None,28,28,32]\n",
    "    x_relu1 = tf.nn.relu(tf.nn.conv2d(x_reshape, w_conv1, strides=[1,1,1,1], padding=\"SAME\")+b_conv1)\n",
    "    \n",
    "    # 池化窗口=2*2, strides=2 [None, 28, 28, 32] --> [None, 14, 14, 32]\n",
    "    x_pool1 = tf.nn.max_pool(x_relu1,ksize=[1,2,2,1],strides=[1,2,2,1],padding=\"SAME\")\n",
    "    \n",
    "# 3. 二卷积层卷积：Filter=5*5*32, 64个filter，strides=1\n",
    "with tf.variable_scope(\"conv2\"):\n",
    "    # 随机初始化卷积Filter W [5, 5, 32, 64] bias=[64]\n",
    "    w_conv2 = tf.Variable(tf.random_normal([5,5,32,64],mean=0.0,stddev=1.0))\n",
    "    b_conv2 = tf.Variable(tf.constant(0.0, shape=[64]))\n",
    "    \n",
    "    # [None 14,14,32] ==> [None,14,14,64]\n",
    "    x_relu2 = tf.nn.relu(tf.nn.conv2d(x_pool1,w_conv2,strides=[1,1,1,1],padding=\"SAME\")+b_conv2)\n",
    "    \n",
    "    # 池化窗口：2*2 strides:2 [None, 14, 14,64] --> [None,7,7,64]\n",
    "    x_pool2 = tf.nn.max_pool(x_relu2,ksize=[1,2,2,1],strides=[1,2,2,1],padding=\"SAME\")\n",
    "    \n",
    "# 4. 全连接层 [None, 7, 7, 64]  -->\n",
    "# [None, 7*7*64]*[7*7*64, 10]+[10] = [None, 10]\n",
    "with tf.variable_scope(\"fc\"):\n",
    "    w_fc = tf.Variable(tf.random_normal([7*7*64, 10],mean=0.0,stddev=1.0))\n",
    "    b_fc = tf.Variable(tf.constant(0.0,shape=[10]))\n",
    "\n",
    "    #修改x 形状 [None,7,7,64] --》[None, 7*7*64]\n",
    "    x_fc_reshape = tf.reshape(x_pool2, [-1,7*7*64])\n",
    "\n",
    "    # 进行矩阵运算，得出每个样本的10个结果\n",
    "    y_predict = tf.matmul(x_fc_reshape, w_fc) + b_fc\n",
    "#     print(y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. 进行交叉熵损失计算\n",
    "# 求出所有样本的损失，然后求平均值\n",
    "with tf.variable_scope(\"soft_cross\"):\n",
    "    fc_entropy = tf.nn.softmax_cross_entropy_with_logits(labels=y_true, logits=y_predict)\n",
    "#     print(\"fc_entropy:\", fc_entropy)\n",
    "    \n",
    "    loss = tf.reduce_mean(fc_entropy)\n",
    "#     print(loss)\n",
    "\n",
    "# 6. 梯度下降求出损失\n",
    "with tf.variable_scope(\"optimizer\"):\n",
    "    train_op = tf.train.GradientDescentOptimizer(0.0001).minimize(loss)\n",
    "    \n",
    "# 7. 计算准确率\n",
    "with tf.variable_scope(\"acc\"):\n",
    "    equal_list = tf.equal(tf.argmax(y_true,1),tf.arg_max(y_predict,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(equal_list, tf.float32))\n",
    "#     print(accuracy)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 step,acc:0.180000\n",
      "100 step,acc:0.400000\n",
      "200 step,acc:0.460000\n",
      "300 step,acc:0.500000\n",
      "400 step,acc:0.740000\n",
      "500 step,acc:0.660000\n",
      "600 step,acc:0.700000\n",
      "700 step,acc:0.600000\n",
      "800 step,acc:0.700000\n",
      "900 step,acc:0.780000\n",
      "1000 step,acc:0.760000\n",
      "1100 step,acc:0.780000\n",
      "1200 step,acc:0.820000\n",
      "1300 step,acc:0.860000\n",
      "1400 step,acc:0.880000\n",
      "1500 step,acc:0.780000\n",
      "1600 step,acc:0.820000\n",
      "1700 step,acc:0.800000\n",
      "1800 step,acc:0.800000\n",
      "1900 step,acc:0.780000\n",
      "2000 step,acc:0.880000\n",
      "2100 step,acc:0.840000\n",
      "2200 step,acc:0.820000\n",
      "2300 step,acc:0.720000\n",
      "2400 step,acc:0.880000\n",
      "2500 step,acc:0.960000\n",
      "2600 step,acc:0.780000\n",
      "2700 step,acc:0.840000\n",
      "2800 step,acc:0.860000\n",
      "2900 step,acc:0.780000\n",
      "3000 step,acc:0.820000\n",
      "3100 step,acc:0.820000\n",
      "3200 step,acc:0.940000\n",
      "3300 step,acc:0.820000\n",
      "3400 step,acc:0.820000\n",
      "3500 step,acc:0.880000\n",
      "3600 step,acc:0.860000\n",
      "3700 step,acc:0.820000\n",
      "3800 step,acc:0.880000\n",
      "3900 step,acc:0.860000\n",
      "4000 step,acc:0.860000\n",
      "4100 step,acc:0.860000\n",
      "4200 step,acc:0.800000\n",
      "4300 step,acc:0.860000\n",
      "4400 step,acc:0.880000\n",
      "4500 step,acc:0.940000\n",
      "4600 step,acc:0.940000\n",
      "4700 step,acc:0.860000\n",
      "4800 step,acc:0.840000\n",
      "4900 step,acc:0.900000\n",
      "5000 step,acc:0.840000\n",
      "5100 step,acc:0.920000\n",
      "5200 step,acc:0.800000\n",
      "5300 step,acc:0.880000\n",
      "5400 step,acc:0.900000\n",
      "5500 step,acc:0.840000\n",
      "5600 step,acc:0.860000\n",
      "5700 step,acc:0.920000\n",
      "5800 step,acc:0.880000\n",
      "5900 step,acc:0.880000\n",
      "6000 step,acc:0.900000\n",
      "6100 step,acc:0.920000\n",
      "6200 step,acc:0.920000\n",
      "6300 step,acc:0.920000\n",
      "6400 step,acc:0.820000\n",
      "6500 step,acc:0.840000\n",
      "6600 step,acc:0.880000\n",
      "6700 step,acc:0.980000\n",
      "6800 step,acc:0.900000\n",
      "6900 step,acc:0.840000\n",
      "7000 step,acc:0.900000\n",
      "7100 step,acc:0.860000\n",
      "7200 step,acc:0.900000\n",
      "7300 step,acc:0.940000\n",
      "7400 step,acc:0.940000\n",
      "7500 step,acc:0.920000\n",
      "7600 step,acc:0.840000\n",
      "7700 step,acc:0.960000\n",
      "7800 step,acc:0.920000\n",
      "7900 step,acc:0.840000\n",
      "8000 step,acc:0.900000\n",
      "8100 step,acc:0.820000\n",
      "8200 step,acc:0.940000\n",
      "8300 step,acc:0.840000\n",
      "8400 step,acc:0.920000\n",
      "8500 step,acc:0.860000\n",
      "8600 step,acc:0.720000\n",
      "8700 step,acc:0.920000\n",
      "8800 step,acc:0.960000\n",
      "8900 step,acc:0.980000\n",
      "9000 step,acc:0.940000\n",
      "9100 step,acc:0.860000\n",
      "9200 step,acc:0.840000\n",
      "9300 step,acc:0.940000\n",
      "9400 step,acc:0.840000\n",
      "9500 step,acc:0.920000\n",
      "9600 step,acc:0.900000\n",
      "9700 step,acc:0.940000\n",
      "9800 step,acc:0.940000\n",
      "9900 step,acc:0.980000\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'float' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-60-8fadc577ca1b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%d step,acc:%f\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mmnist_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mmnist_y\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstart_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'float' object is not callable"
     ]
    }
   ],
   "source": [
    "#定义一个初始化变量OP\n",
    "init_op = tf.global_variables_initializer()\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init_op)\n",
    "    start_time = time.time()\n",
    "    # 循环训练\n",
    "    for i in range(10000):\n",
    "        mnist_x, mnist_y = mnist.train.next_batch(50)\n",
    "        sess.run(train_op,feed_dict={x:mnist_x,y_true:mnist_y})\n",
    "        \n",
    "        if i %100 == 0:\n",
    "            print(\"%d step,acc:%f\" %(i, sess.run(accuracy, feed_dict={x:mnist_x,y_true:mnist_y})))\n",
    "    \n",
    "    print(time.time()-start_time())\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function softmax_cross_entropy_with_logits in module tensorflow.python.ops.nn_ops:\n",
      "\n",
      "softmax_cross_entropy_with_logits(_sentinel=None, labels=None, logits=None, dim=-1, name=None, axis=None)\n",
      "    Computes softmax cross entropy between `logits` and `labels`. (deprecated)\n",
      "    \n",
      "    Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.\n",
      "    Instructions for updating:\n",
      "    \n",
      "    Future major versions of TensorFlow will allow gradients to flow\n",
      "    into the labels input on backprop by default.\n",
      "    \n",
      "    See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "    \n",
      "    \n",
      "    Measures the probability error in discrete classification tasks in which the\n",
      "    classes are mutually exclusive (each entry is in exactly one class).  For\n",
      "    example, each CIFAR-10 image is labeled with one and only one label: an image\n",
      "    can be a dog or a truck, but not both.\n",
      "    \n",
      "    **NOTE:**  While the classes are mutually exclusive, their probabilities\n",
      "    need not be.  All that is required is that each row of `labels` is\n",
      "    a valid probability distribution.  If they are not, the computation of the\n",
      "    gradient will be incorrect.\n",
      "    \n",
      "    If using exclusive `labels` (wherein one and only\n",
      "    one class is true at a time), see `sparse_softmax_cross_entropy_with_logits`.\n",
      "    \n",
      "    **WARNING:** This op expects unscaled logits, since it performs a `softmax`\n",
      "    on `logits` internally for efficiency.  Do not call this op with the\n",
      "    output of `softmax`, as it will produce incorrect results.\n",
      "    \n",
      "    A common use case is to have logits and labels of shape\n",
      "    `[batch_size, num_classes]`, but higher dimensions are supported, with\n",
      "    the `dim` argument specifying the class dimension.\n",
      "    \n",
      "    Backpropagation will happen only into `logits`.  To calculate a cross entropy\n",
      "    loss that allows backpropagation into both `logits` and `labels`, see\n",
      "    `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "    \n",
      "    **Note that to avoid confusion, it is required to pass only named arguments to\n",
      "    this function.**\n",
      "    \n",
      "    Args:\n",
      "      _sentinel: Used to prevent positional parameters. Internal, do not use.\n",
      "      labels: Each vector along the class dimension should hold a valid\n",
      "        probability distribution e.g. for the case in which labels are of shape\n",
      "        `[batch_size, num_classes]`, each row of `labels[i]` must be a valid\n",
      "        probability distribution.\n",
      "      logits: Per-label activations, typically a linear output. These activation\n",
      "        energies are interpreted as unnormalized log probabilities.\n",
      "      dim: The class dimension. Defaulted to -1 which is the last dimension.\n",
      "      name: A name for the operation (optional).\n",
      "      axis: Alias for dim.\n",
      "    \n",
      "    Returns:\n",
      "      A `Tensor` that contains the softmax cross entropy loss. Its type is the\n",
      "      same as `logits` and its shape is the same as `labels` except that it does\n",
      "      not have the last dimension of `labels`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "help(tf.nn.softmax_cross_entropy_with_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0023791790008545\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "a = time.time()\n",
    "\n",
    "time.sleep(2)\n",
    "print(time.time() -a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function max_pool in module tensorflow.python.ops.nn_ops:\n",
      "\n",
      "max_pool(value, ksize, strides, padding, data_format='NHWC', name=None, input=None)\n",
      "    Performs the max pooling on the input.\n",
      "    \n",
      "    Args:\n",
      "      value: A 4-D `Tensor` of the format specified by `data_format`.\n",
      "      ksize: An int or list of `ints` that has length `1`, `2` or `4`.\n",
      "        The size of the window for each dimension of the input tensor.\n",
      "      strides: An int or list of `ints` that has length `1`, `2` or `4`.\n",
      "        The stride of the sliding window for each dimension of the input tensor.\n",
      "      padding: A string, either `'VALID'` or `'SAME'`. The padding algorithm.\n",
      "        See the \"returns\" section of `tf.nn.convolution` for details.\n",
      "      data_format: A string. 'NHWC', 'NCHW' and 'NCHW_VECT_C' are supported.\n",
      "      name: Optional name for the operation.\n",
      "      input: Alias for value.\n",
      "    \n",
      "    Returns:\n",
      "      A `Tensor` of format specified by `data_format`.\n",
      "      The max pooled output tensor.\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
